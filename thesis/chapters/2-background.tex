%% Background
\chapter{Background}
\label{chap:background}

\section{Neural Machine Translation and the Transformer Model}

Neural networks be adapted for many different tasks, and those tasks can be categorized under various labels which describe the type of input and the type of output. For example, spam detection would fall under “sequence classification.” Translation is a sequence-to-sequence task, because the input sequence (e.g. a sentence in French) is being used to generate an output sequence (e.g. an English translation of that sentence). The input and output sequences can and often do differ from one another in length, in script, and in sentence structure. 

How would it work to individually translate words in a source sentence and combine them according to the target language’s syntax rules? It would often provide a terrible translation, not just because different languages can communicate the same information in more or fewer words than one another; words cannot be translated individually because their meanings depend on their context. The same preposition, “in,” may correspond to any of several German prepositions – “in” or “auf” or “bei” – depending on the context. Thus effective translation of a single word in context may require use of an entire sentence or more than one sentence. 

