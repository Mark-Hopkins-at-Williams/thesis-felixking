%% Background
\chapter{Background}
\label{chap:background}

\section{Neural Machine Translation and the Transformer Model}

Neural networks can be adapted for many different tasks, and those tasks can be categorized under 
labels which describe the type of input and the type of output. For example, spam detection would 
fall under “sequence classification.” Translation is a sequence-to-sequence task, because the input 
sequence (e.g. a sentence in French) is being used to generate an output sequence (e.g. an English 
translation of that sentence). The input and output sequences can and often do differ from one another 
in length, in script, and in sentence structure. 

How would it work to individually translate words in a source sentence and combine them according to 
the target language’s syntax rules? It would often provide a terrible translation, not just because 
different languages can communicate the same information in more or fewer words than one another; words 
cannot be translated individually because their meanings depend on their context. The same preposition, 
“in,” may correspond to any of several German prepositions – “in” or “auf” or “bei” – depending on the 
context. Thus effective translation of a single word in context may require use of an entire sentence or 
more than one sentence. 

something in here about early NMT methods

The Transformer model (2017, cite) innovated on the encoder-decoder structure (cite), and it is still the standard for NMT 
architecture and other applications. It uses an attention (cite attention paper?) and feedforward layers in place of recurrent components. 
This new architecture not only has some features suitable to many linguistic tasks, but trains much more quickly 
than the alternatives as it allows for far more parallel computation in training. 

In the following section, I'll describe at a high level how a Transformer model translates a sentence. 

Consider the English sentence "The man wearing black shoes rode his bicycle."

\vskip.25in
Step 1: Tokenize

The sentence must first be broken down into small parts for the model to work with. This is done using a tokenizer, which is
often itself a trained component of the model. The tokenizer's vocabulary contains enough words and sub-words to cover the languages
the model takes as input. Thus the tokenizer turns the sentence into a list of tokens, which are all elements in the tokenizer's
vocabulary. Here is the result of tokenizing the above sentence using the tokenizer in Facebook's NLLB models:
\vskip.25in

    [256047, 1617, 492, 214030, 49154, 203020, 134457, 4414, 330, 163731, 248075, 2]
    
    ['eng\_Latn', 'The', 'man', 'wearing', 'black', 'shoes', 'rode', 'his', 'bi', 'cycle', '.', '</s>']

\vskip.25in
The numerical array just desplays the indices of the tokens in the tokenizer's vocabulary. Note '</s>,' which did not appear 
in the original sentence; it is a special "end of sentence" token which the tokenizer adds to every input.

\vskip.25in
Step 2: Encode

The encoder layers take the tokenized input ...